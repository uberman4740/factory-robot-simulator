{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import q_learner\n",
    "from q_learner import QLearner, QNetwork\n",
    "from labeling_network import FullyConnectedLayer, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer = FullyConnectedLayer(3, 2, activation_fn=linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MB_SIZE = 20\n",
    "GAMMA = 0.95\n",
    "BURN_IN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_function = QNetwork([layer], minibatch_size=MB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_learner = QLearner(q_function,\n",
    "                    exp_store_size=10000,\n",
    "                    percept_length=3,\n",
    "                    n_actions=2,\n",
    "                    state_stm=1,\n",
    "                    gamma=GAMMA,\n",
    "                    minibatch_size=MB_SIZE,\n",
    "                    prng=np.random.RandomState(12345678))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPD-Testclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    \n",
    "    def __init__(self, states, actions, transitions, rewards, init_state):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.current_state = init_state\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        if (self.current_state, action) in self.transitions:\n",
    "            ts = self.transitions[(self.current_state, action)]\n",
    "            self.current_state = self.sample_state(ts)\n",
    "    \n",
    "    \n",
    "    def add_state(self, state):\n",
    "        if not state in self.states:\n",
    "            self.states.append(state)\n",
    "        \n",
    "        \n",
    "    def add_action(self, action):\n",
    "        if not action in self.actions:\n",
    "            self.actions.append(action)\n",
    "        \n",
    "        \n",
    "    def add_transition(self, f, a, ts):\n",
    "        self.transitions[(f, a)] = ts\n",
    "    \n",
    "    \n",
    "    def add_reward(self, f, a, t, r):\n",
    "        self.rewards[(f, a, t)] = r\n",
    "    \n",
    "    \n",
    "    def get_reward(self, f, a, t):\n",
    "        if (f, a, t) in self.rewards:\n",
    "            return self.rewards[(f, a, t)]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def sample_state(self, ts):\n",
    "        choice = np.random.uniform(0,1)\n",
    "        i = 0\n",
    "        while choice > 0:\n",
    "            choice -= ts[i][1]\n",
    "            i += 1\n",
    "        return ts[i-1][0]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate MDPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# states = ['q0', 'q1']\n",
    "# actions = ['a0', 'a1']\n",
    "\n",
    "# mdp = MDP(states, actions, {}, {}, 'q0')\n",
    "\n",
    "\n",
    "# mdp.add_transition('q0', 'a0', [('q0', 0.8), ('q1', 0.2)])\n",
    "# mdp.add_transition('q0', 'a1', [('q0', 1.0)])\n",
    "\n",
    "# mdp.add_transition('q1', 'a0', [('q0', 0.1), ('q1', 0.9)])\n",
    "# mdp.add_transition('q1', 'a1', [('q0', 1.0)])\n",
    "\n",
    "\n",
    "# mdp.add_reward('q0', 'a0', 'q0', -1.0)\n",
    "# mdp.add_reward('q0', 'a1', 'q0',  1.0)\n",
    "# mdp.add_reward('q1', 'a0', 'q1',  4.0)\n",
    "# mdp.add_reward('q1', 'a1', 'q0',  5.0)\n",
    "\n",
    "\n",
    "# state_transl = dict()\n",
    "# state_transl['q0'] = [1.0, 0.0]\n",
    "# state_transl['q1'] = [0.0, 1.0]\n",
    "\n",
    "# action_transl = dict()\n",
    "# action_transl['a0'] = 0\n",
    "# action_transl['a1'] = 1\n",
    "\n",
    "# action_inv_transl = dict((v, k) for k, v in action_transl.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = ['q0', 'q1', 'q2']\n",
    "actions = ['a0', 'a1']\n",
    "\n",
    "mdp = MDP(states, actions, {}, {}, 'q0')\n",
    "\n",
    "\n",
    "mdp.add_transition('q0', 'a0', [('q0', 0.8), ('q1', 0.2)])\n",
    "mdp.add_transition('q0', 'a1', [('q0', 0.8), ('q1', 0.2)])\n",
    "\n",
    "mdp.add_transition('q1', 'a0', [('q0', 0.40), ('q1', 0.60)])\n",
    "mdp.add_transition('q1', 'a1', [('q0', 0.10), ('q2', 0.90)])\n",
    "\n",
    "mdp.add_transition('q2', 'a0', [('q0', 0.30), ('q2', 0.70)])\n",
    "mdp.add_transition('q2', 'a1', [('q1', 0.50), ('q2', 0.50)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mdp.add_reward('q0', 'a0', 'q0', -5.0)\n",
    "mdp.add_reward('q0', 'a0', 'q1', -5.0)\n",
    "\n",
    "mdp.add_reward('q0', 'a1', 'q0', -5.0)\n",
    "mdp.add_reward('q0', 'a1', 'q1', -5.0)\n",
    "\n",
    "mdp.add_reward('q1', 'a0', 'q0',  2.0)\n",
    "mdp.add_reward('q1', 'a0', 'q1',  2.0)\n",
    "\n",
    "mdp.add_reward('q1', 'a1', 'q0',  1.0)\n",
    "mdp.add_reward('q1', 'a1', 'q2',  0.0)\n",
    "\n",
    "mdp.add_reward('q2', 'a0', 'q0',  1.0)\n",
    "mdp.add_reward('q2', 'a0', 'q2', 10.0)\n",
    "\n",
    "mdp.add_reward('q2', 'a1', 'q1',  0.0)\n",
    "mdp.add_reward('q2', 'a1', 'q2',  4.0)\n",
    "\n",
    "\n",
    "state_transl = dict()\n",
    "state_transl['q0'] = [1.0, 0.0, 0.0]\n",
    "state_transl['q1'] = [0.0, 1.0, 0.0]\n",
    "state_transl['q2'] = [0.0, 0.0, 1.0]\n",
    "\n",
    "action_transl = dict()\n",
    "action_transl['a0'] = 0\n",
    "action_transl['a1'] = 1\n",
    "\n",
    "action_inv_transl = dict((v, k) for k, v in action_transl.iteritems())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Q-Learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth(a, smoothn=10):\n",
    "    b = [np.mean(a[k-smoothn:k+smoothn]) for k in range(smoothn, len(a)-smoothn)]\n",
    "    return np.asarray(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_ITERATIONS = 100000\n",
    "N_TRAIN_ITERATIONS_PER_IT = 4\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.00\n",
    "epsilon_decrease_duration = 90000\n",
    "\n",
    "\n",
    "costs = []\n",
    "q_values = []\n",
    "\n",
    "total_rewards = [0]\n",
    "action_log = []\n",
    "\n",
    "\n",
    "alt_learning_rate = 0.01\n",
    "alt_qs = dict()\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        alt_qs[(s, a)] = 0.\n",
    "        \n",
    "\n",
    "start_time = time.time()\n",
    "for i in xrange(N_ITERATIONS):\n",
    "    epsilon = max(epsilon_end, \n",
    "                  epsilon_start - 1.0*i*(epsilon_start - epsilon_end)/epsilon_decrease_duration)\n",
    "    \n",
    "        \n",
    "    \n",
    "    last_state = mdp.current_state\n",
    "    \n",
    "    if np.random.uniform(0,1) < epsilon or i < BURN_IN:\n",
    "        action = np.random.choice(mdp.actions)\n",
    "    else:\n",
    "        action_id = q_learner.get_current_best_action()\n",
    "        action = action_inv_transl[action_id]\n",
    "        \n",
    "    mdp.step(action)\n",
    "    \n",
    "    previous_reward = mdp.get_reward(last_state, action, mdp.current_state)\n",
    "    \n",
    "    total_rewards.append(total_rewards[-1] + previous_reward)\n",
    "    action_log.append(action)\n",
    "    \n",
    "    q_learner.add_observation(state_transl[mdp.current_state], \n",
    "                              action_transl[action], previous_reward)\n",
    "    \n",
    "    if i >= BURN_IN:\n",
    "        for j in xrange(N_TRAIN_ITERATIONS_PER_IT):\n",
    "            cost = q_learner.train_q_function(0.0001)\n",
    "            costs.append(cost)\n",
    "    \n",
    "    #evaluate all Q-values\n",
    "    q_values.append(np.ndarray.flatten(\n",
    "            np.asarray([q_learner.q_function.get_q_values(state_transl[s]) for s in states])))\n",
    "    \n",
    "\n",
    "    best_next_q = np.max([alt_qs[(mdp.current_state, a)] for a in actions])\n",
    "    alt_qs[(last_state, action)] = (1. - alt_learning_rate)*alt_qs[(last_state, action)] + \\\n",
    "                                    alt_learning_rate*(previous_reward + GAMMA*best_next_q)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print 'Time per 1000 iterations: %f s' % (1000.0*(end_time - start_time) / N_ITERATIONS)\n",
    "print 'Mean cost: %f' % (np.mean(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Mean Q-error (smoothed)')\n",
    "plt.plot(smooth(costs, smoothn=20))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('All Q-values')\n",
    "q_labels = np.ndarray.flatten(np.asarray([s + ';' + a for s in states for a in actions ]))\n",
    "for qs, lbl in zip(np.transpose(q_values), q_labels):\n",
    "    ax.plot(qs, label=lbl)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Total reward')\n",
    "plt.plot(total_rewards)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \n",
    "for q in states:\n",
    "    print q + ':', q_learner.q_function.get_q_values(state_transl[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in alt_qs.iterkeys():\n",
    "    print k, ':', alt_qs[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.arange(4*14).reshape(14,4)\n",
    "print t\n",
    "print\n",
    "print t[[2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.array([2, 3, 5, 9, 7])\n",
    "state_stm = 3\n",
    "aug_indices = np.asarray([indices + i \n",
    "                                  for i in range(state_stm)]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_stm = 5\n",
    "exp_counter = 2\n",
    "\n",
    "print np.append(t[exp_counter-state_stm:], t[:exp_counter])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
