{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (15.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import q_learner\n",
    "from q_learner import QLearner, QNetwork\n",
    "from labeling_network import FullyConnectedLayer, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer = FullyConnectedLayer(2, 2, activation_fn=linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MB_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_function = QNetwork([layer], minibatch_size=MB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_learner = QLearner(q_function,\n",
    "                    exp_store_size=10000,\n",
    "                    percept_length=2,\n",
    "                    n_actions=2,\n",
    "                    state_stm=1,\n",
    "                    gamma=0.90,\n",
    "                    minibatch_size=MB_SIZE,\n",
    "                    prng=np.random.RandomState(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPD-Testclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    \n",
    "    def __init__(self, states, actions, transitions, rewards, init_state):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.current_state = init_state\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        if (self.current_state, action) in self.transitions:\n",
    "            ts = self.transitions[(self.current_state, action)]\n",
    "            self.current_state = self.sample_state(ts)\n",
    "    \n",
    "    \n",
    "    def add_state(self, state):\n",
    "        if not state in self.states:\n",
    "            self.states.append(state)\n",
    "        \n",
    "        \n",
    "    def add_action(self, action):\n",
    "        if not action in self.actions:\n",
    "            self.actions.append(action)\n",
    "        \n",
    "        \n",
    "    def add_transition(self, f, a, ts):\n",
    "        self.transitions[(f, a)] = ts\n",
    "    \n",
    "    \n",
    "    def add_reward(self, f, a, t, r):\n",
    "        self.rewards[(f, a, t)] = r\n",
    "    \n",
    "    \n",
    "    def get_reward(self, f, a, t):\n",
    "        if (f, a, t) in self.rewards:\n",
    "            return self.rewards[(f, a, t)]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def sample_state(self, ts):\n",
    "        choice = np.random.uniform(0,1)\n",
    "        i = 0\n",
    "        while choice > 0:\n",
    "            choice -= ts[i][1]\n",
    "            i += 1\n",
    "        return ts[i-1][0]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate MDPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = ['q0', 'q1']\n",
    "actions = ['a0', 'a1']\n",
    "\n",
    "mdp = MDP(states, actions, {}, {}, 'q0')\n",
    "\n",
    "\n",
    "mdp.add_transition('q0', 'a0', [('q0', 0.8), ('q1', 0.2)])\n",
    "mdp.add_transition('q0', 'a1', [('q0', 1.0)])\n",
    "\n",
    "mdp.add_transition('q1', 'a0', [('q0', 0.1), ('q1', 0.9)])\n",
    "mdp.add_transition('q1', 'a1', [('q0', 1.0)])\n",
    "\n",
    "\n",
    "mdp.add_reward('q0', 'a0', 'q0', -1.0)\n",
    "mdp.add_reward('q0', 'a1', 'q0',  1.0)\n",
    "mdp.add_reward('q1', 'a0', 'q1',  4.0)\n",
    "mdp.add_reward('q1', 'a1', 'q0',  5.0)\n",
    "\n",
    "\n",
    "state_transl = dict()\n",
    "state_transl['q0'] = [1.0, 0.0]\n",
    "state_transl['q1'] = [0.0, 1.0]\n",
    "\n",
    "action_transl = dict()\n",
    "action_transl['a0'] = 0\n",
    "action_transl['a1'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Q-Learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth(a, smoothn=10):\n",
    "    b = [np.mean(a[k-smoothn:k+smoothn]) for k in range(smoothn, len(a)-smoothn)]\n",
    "    return np.asarray(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_ITERATIONS = 50000\n",
    "\n",
    "\n",
    "\n",
    "costs = []\n",
    "q_values = []\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(N_ITERATIONS):\n",
    "    last_state = mdp.current_state\n",
    "\n",
    "    action = np.random.choice(mdp.actions)\n",
    "    mdp.step(action)\n",
    "    \n",
    "    previous_reward = mdp.get_reward(last_state, action, mdp.current_state)\n",
    "    \n",
    "    q_learner.add_observation(state_transl[mdp.current_state], \n",
    "                              action_transl[action], previous_reward)\n",
    "    \n",
    "    cost = q_learner.train_q_function(0.01)\n",
    "    costs.append(cost)\n",
    "    \n",
    "    #evaluate all Q-values\n",
    "    q_values.append(np.ndarray.flatten(\n",
    "            np.asarray([q_learner.q_function.get_q_values(state_transl[s]) for s in states])))\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print 'Time per 1000 iterations: %f s' % (1000.0*(end_time - start_time) / N_ITERATIONS)\n",
    "print 'Mean cost: %f' % (np.mean(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Mean Q-error (smoothed)')\n",
    "plt.plot(smooth(costs, smoothn=20))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('All Q-values')\n",
    "q_labels = np.ndarray.flatten(np.asarray([s + ';' + a for s in states for a in actions ]))\n",
    "for qs, lbl in zip(np.transpose(q_values), q_labels):\n",
    "    ax.plot(qs, label=lbl)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \n",
    "for q in states:\n",
    "    print q + ':', q_learner.q_function.get_q_values(state_transl[q])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
