{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dataloader\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import TimeDistributedDense, Activation, Dropout\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, JZS1, GRU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "def save_list(l, filepath):\n",
    "    f = open(filepath, 'w')\n",
    "    for item in l:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatX(a):\n",
    "    return np.asarray(a, dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data (1250000)...\n",
      "build model...\n",
      "Compile time: 3.14011502266 sec\n",
      "start training...\n",
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/400\n",
      "2160/2160 [==============================] - 7s - loss: -0.3670 - val_loss: -0.4151\n",
      "Epoch 2/400\n",
      "2160/2160 [==============================] - 7s - loss: -0.4631 - val_loss: -0.5150\n",
      "Epoch 3/400\n",
      "2160/2160 [==============================] - 7s - loss: -0.5730 - val_loss: -0.6385\n",
      "Epoch 4/400\n",
      "2160/2160 [==============================] - 7s - loss: -0.7158 - val_loss: -0.8056\n",
      "Epoch 5/400\n",
      "2160/2160 [==============================] - 7s - loss: -0.9153 - val_loss: -1.0440\n",
      "Epoch 6/400\n",
      "2160/2160 [==============================] - 7s - loss: -1.1962 - val_loss: -1.3567\n",
      "Epoch 7/400\n",
      "2160/2160 [==============================] - 7s - loss: -1.5087 - val_loss: -1.6335\n",
      "Epoch 8/400\n",
      "2160/2160 [==============================] - 7s - loss: -1.7558 - val_loss: -1.8436\n",
      "Epoch 9/400\n",
      "2160/2160 [==============================] - 7s - loss: -1.9559 - val_loss: -2.0318\n",
      "Epoch 10/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.1374 - val_loss: -2.2040\n",
      "Epoch 11/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.2991 - val_loss: -2.3546\n",
      "Epoch 12/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.4404 - val_loss: -2.4858\n",
      "Epoch 13/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.5622 - val_loss: -2.5938\n",
      "Epoch 14/400\n",
      "2160/2160 [==============================] - 8s - loss: -2.6571 - val_loss: -2.6731\n",
      "Epoch 15/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.7283 - val_loss: -2.7341\n",
      "Epoch 16/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.7830 - val_loss: -2.7797\n",
      "Epoch 17/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.8253 - val_loss: -2.8178\n",
      "Epoch 18/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.8640 - val_loss: -2.8548\n",
      "Epoch 19/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.9022 - val_loss: -2.8916\n",
      "Epoch 20/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.9402 - val_loss: -2.9281\n",
      "Epoch 21/400\n",
      "2160/2160 [==============================] - 7s - loss: -2.9777 - val_loss: -2.9638\n",
      "Epoch 22/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.0140 - val_loss: -2.9980\n",
      "Epoch 23/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.0485 - val_loss: -3.0302\n",
      "Epoch 24/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.0807 - val_loss: -3.0602\n",
      "Epoch 25/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.1104 - val_loss: -3.0879\n",
      "Epoch 26/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.1378 - val_loss: -3.1135\n",
      "Epoch 27/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.1632 - val_loss: -3.1372\n",
      "Epoch 28/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.1865 - val_loss: -3.1589\n",
      "Epoch 29/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.2079 - val_loss: -3.1787\n",
      "Epoch 30/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.2275 - val_loss: -3.1968\n",
      "Epoch 31/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.2457 - val_loss: -3.2138\n",
      "Epoch 32/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.2628 - val_loss: -3.2297\n",
      "Epoch 33/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.2791 - val_loss: -3.2449\n",
      "Epoch 34/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.2947 - val_loss: -3.2595\n",
      "Epoch 35/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.3099 - val_loss: -3.2736\n",
      "Epoch 36/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.3246 - val_loss: -3.2873\n",
      "Epoch 37/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.3389 - val_loss: -3.3005\n",
      "Epoch 38/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.3529 - val_loss: -3.3133\n",
      "Epoch 39/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.3665 - val_loss: -3.3258\n",
      "Epoch 40/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.3799 - val_loss: -3.3380\n",
      "Epoch 41/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.3929 - val_loss: -3.3499\n",
      "Epoch 42/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4057 - val_loss: -3.3615\n",
      "Epoch 43/400\n",
      "2160/2160 [==============================] - 8s - loss: -3.4183 - val_loss: -3.3728\n",
      "Epoch 44/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4306 - val_loss: -3.3838\n",
      "Epoch 45/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4427 - val_loss: -3.3945\n",
      "Epoch 46/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4545 - val_loss: -3.4050\n",
      "Epoch 47/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4662 - val_loss: -3.4153\n",
      "Epoch 48/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4776 - val_loss: -3.4252\n",
      "Epoch 49/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4888 - val_loss: -3.4350\n",
      "Epoch 50/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.4998 - val_loss: -3.4445\n",
      "Epoch 51/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.5106 - val_loss: -3.4537\n",
      "Epoch 52/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.5211 - val_loss: -3.4628\n",
      "Epoch 53/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.5315 - val_loss: -3.4715\n",
      "Epoch 54/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.5417 - val_loss: -3.4801\n",
      "Epoch 55/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.5516 - val_loss: -3.4884\n",
      "Epoch 56/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.5614 - val_loss: -3.4965\n",
      "Epoch 57/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.5709 - val_loss: -3.5043\n",
      "Epoch 58/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.5802 - val_loss: -3.5119\n",
      "Epoch 59/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.5894 - val_loss: -3.5193\n",
      "Epoch 60/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.5984 - val_loss: -3.5265\n",
      "Epoch 61/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.6072 - val_loss: -3.5334\n",
      "Epoch 62/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.6158 - val_loss: -3.5402\n",
      "Epoch 63/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.6243 - val_loss: -3.5468\n",
      "Epoch 64/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.6326 - val_loss: -3.5531\n",
      "Epoch 65/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.6407 - val_loss: -3.5593\n",
      "Epoch 66/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.6488 - val_loss: -3.5654\n",
      "Epoch 67/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.6567 - val_loss: -3.5712\n",
      "Epoch 68/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.6644 - val_loss: -3.5769\n",
      "Epoch 69/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.6721 - val_loss: -3.5824\n",
      "Epoch 70/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.6796 - val_loss: -3.5879\n",
      "Epoch 71/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.6871 - val_loss: -3.5931\n",
      "Epoch 72/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.6944 - val_loss: -3.5983\n",
      "Epoch 73/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.7017 - val_loss: -3.6033\n",
      "Epoch 74/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.7089 - val_loss: -3.6082\n",
      "Epoch 75/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.7160 - val_loss: -3.6130\n",
      "Epoch 76/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.7230 - val_loss: -3.6177\n",
      "Epoch 77/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.7300 - val_loss: -3.6223\n",
      "Epoch 78/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.7369 - val_loss: -3.6268\n",
      "Epoch 79/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7437 - val_loss: -3.6312\n",
      "Epoch 80/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.7505 - val_loss: -3.6356\n",
      "Epoch 81/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7572 - val_loss: -3.6398\n",
      "Epoch 82/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7639 - val_loss: -3.6440\n",
      "Epoch 83/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7705 - val_loss: -3.6481\n",
      "Epoch 84/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7771 - val_loss: -3.6521\n",
      "Epoch 85/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7836 - val_loss: -3.6560\n",
      "Epoch 86/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7902 - val_loss: -3.6599\n",
      "Epoch 87/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.7966 - val_loss: -3.6637\n",
      "Epoch 88/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8031 - val_loss: -3.6674\n",
      "Epoch 89/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8095 - val_loss: -3.6711\n",
      "Epoch 90/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8158 - val_loss: -3.6747\n",
      "Epoch 91/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8222 - val_loss: -3.6782\n",
      "Epoch 92/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.8285 - val_loss: -3.6817\n",
      "Epoch 93/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8348 - val_loss: -3.6851\n",
      "Epoch 94/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8410 - val_loss: -3.6884\n",
      "Epoch 95/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.8472 - val_loss: -3.6917\n",
      "Epoch 96/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8534 - val_loss: -3.6948\n",
      "Epoch 97/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8596 - val_loss: -3.6979\n",
      "Epoch 98/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8657 - val_loss: -3.7009\n",
      "Epoch 99/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8717 - val_loss: -3.7038\n",
      "Epoch 100/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8778 - val_loss: -3.7065\n",
      "Epoch 101/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8837 - val_loss: -3.7092\n",
      "Epoch 102/400\n",
      "2160/2160 [==============================] - 8s - loss: -3.8896 - val_loss: -3.7117\n",
      "Epoch 103/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.8955 - val_loss: -3.7141\n",
      "Epoch 104/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.9013 - val_loss: -3.7164\n",
      "Epoch 105/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.9070 - val_loss: -3.7184\n",
      "Epoch 106/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9127 - val_loss: -3.7203\n",
      "Epoch 107/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9182 - val_loss: -3.7221\n",
      "Epoch 108/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9237 - val_loss: -3.7236\n",
      "Epoch 109/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9291 - val_loss: -3.7249\n",
      "Epoch 110/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9344 - val_loss: -3.7259\n",
      "Epoch 111/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9396 - val_loss: -3.7268\n",
      "Epoch 112/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9447 - val_loss: -3.7273\n",
      "Epoch 113/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9497 - val_loss: -3.7276\n",
      "Epoch 114/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9546 - val_loss: -3.7276\n",
      "Epoch 115/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.9595 - val_loss: -3.7273\n",
      "Epoch 116/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.9642 - val_loss: -3.7267\n",
      "Epoch 117/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.9688 - val_loss: -3.7258\n",
      "Epoch 118/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.9733 - val_loss: -3.7245\n",
      "Epoch 119/400\n",
      "2160/2160 [==============================] - 6s - loss: -3.9777 - val_loss: -3.7229\n",
      "Epoch 120/400\n",
      "2160/2160 [==============================] - 7s - loss: -3.9820 - val_loss: -3.7210\n",
      "100/100 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "counter = 10000\n",
    "out_dir = datetime.now().strftime('%Y%m%d') + 'out_' + str(counter) + '/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "rng = np.random.RandomState()\n",
    "\n",
    "# load data\n",
    "sequence_length = 500\n",
    "crop_end = 2\n",
    "n_train = 2400\n",
    "n_test = 100\n",
    "n_data = sequence_length * (n_train + n_test)\n",
    "input_length = 21\n",
    "percept_length = 18\n",
    "\n",
    "print 'load data ({0})...'.format(n_data)\n",
    "data = dataloader.get_data(0, n_data + 1)\n",
    "inputs, percepts = dataloader.make_batches(data, sequence_length, crop_end=crop_end)\n",
    "x_train = inputs[:n_train]\n",
    "y_train = percepts[:n_train]\n",
    "\n",
    "x_test = inputs[n_train:]\n",
    "y_test = percepts[n_train:]\n",
    "\n",
    "n_hidden = 180\n",
    "dropout = False\n",
    "early_stopping_patience = 5\n",
    "n_additional = 180\n",
    "\n",
    "#\n",
    "# n_hidden = rng.choice([20, 40, 80, 160, 320])\n",
    "# dropout = rng.choice([True, False], p=[0.2, 0.8])\n",
    "# early_stopping_patience = rng.choice([10, 20, 50])\n",
    "#\n",
    "# n_additional = rng.choice([0, 40, 60], p=[0.6, 0.2, 0.2])\n",
    "\n",
    "print 'build model...'\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(input_dim=input_length,\n",
    "#                output_dim=n_hidden,\n",
    "#                activation='tanh',\n",
    "#                inner_activation='hard_sigmoid',\n",
    "#                init='glorot_uniform',\n",
    "#                inner_init='orthogonal',\n",
    "#                forget_bias_init='one',\n",
    "#                return_sequences=True))\n",
    "\n",
    "\n",
    "# if n_additional > 0:\n",
    "#     model.add(LSTM(output_dim=n_additional,\n",
    "#                    activation='tanh',\n",
    "#                    inner_activation='hard_sigmoid',\n",
    "#                    init='glorot_uniform',\n",
    "#                    inner_init='orthogonal',\n",
    "#                    forget_bias_init='one',\n",
    "#                    return_sequences=True))\n",
    "\n",
    "\n",
    "# model.add(GRU(output_dim=2 * percept_length,\n",
    "#                activation='sigmoid',\n",
    "#                inner_activation='hard_sigmoid',\n",
    "#                init='glorot_uniform',\n",
    "#                inner_init='orthogonal',\n",
    "#                # forget_bias_init='one',\n",
    "#                return_sequences=True))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributedDense(100,\n",
    "                               input_dim=input_length))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(TimeDistributedDense(2*percept_length))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "# 1/(sqrt(2pi) sigma)  exp(-(x - mu)^2 / (2 sigma^2))\n",
    "#     loss_value = T.exp(-((y_true - y_pred[:, :percept_length]) ** 2) / (2 * ) ) / y_pred[:, percept_length:]   \n",
    "# todo: algebraic simplification\n",
    "# def loss_fn(y_true, y_pred):\n",
    "#     sigmas_pred = y_pred[:, :percept_length]\n",
    "#     mus_pred = y_pred[:, percept_length:]\n",
    "#     return -T.mean(T.log(T.exp(-((y_true - mus_pred) ** 2) / (2 * sigmas_pred ** 2) ) / sigmas_pred))\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    sigmas_pred = y_pred[:, percept_length:]\n",
    "    mus_pred = y_pred[:, :percept_length]\n",
    "    return T.mean(((y_true - mus_pred) ** 2) / (2 * sigmas_pred ** 2) + T.log(sigmas_pred))\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "model.compile(loss=lambda y_true, y_pred: loss_fn(y_true, y_pred), optimizer=SGD(lr=0.001, clipnorm=1.0))\n",
    "compile_time = time.time() - tic\n",
    "print 'Compile time: {0} sec'.format(compile_time)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience)\n",
    "model_checkpoint = ModelCheckpoint(out_dir + 'model_checkpoint.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True)\n",
    "\n",
    "print 'start training...'\n",
    "\n",
    "tic = time.time()\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=1,\n",
    "          nb_epoch=400,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, model_checkpoint],\n",
    "          shuffle=False)\n",
    "training_duration = time.time() - tic\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=4)\n",
    "\n",
    "save_list([n_hidden, dropout, early_stopping_patience, n_additional, compile_time, training_duration, score],\n",
    "          out_dir + '_choice_and_result.dat')\n",
    "\n",
    "json_string = model.to_json()\n",
    "save_list([json_string],\n",
    "          out_dir + 'json.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.        ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 1.10000002  0.89999998]]\n",
      "---\n",
      "[  0.00000000e+00   1.24999984e+05   1.06250048e-02]\n",
      "---\n",
      "[ -1.38155107e+01   1.24993078e+05   1.06250048e-02]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "percept_length = 2\n",
    "def loss_fn(y_true, y_pred):\n",
    "    sigmas_pred = y_pred[:, percept_length:]\n",
    "    mus_pred = y_pred[:, :percept_length]\n",
    "    return (mus_pred,\n",
    "            T.mean(((y_true - mus_pred) ** 2) / (2 * sigmas_pred ** 2), axis=1),\n",
    "            T.mean(((y_true - mus_pred) ** 2) / (2 * sigmas_pred ** 2) + T.log(sigmas_pred), axis=1))\n",
    "\n",
    "\n",
    "y_true = T.matrix('y_true')\n",
    "y_pred = T.matrix('y_pred')\n",
    "\n",
    "for fn in loss_fn(y_true, y_pred):\n",
    "    fn_ = theano.function([y_true, y_pred], fn, on_unused_input='ignore')\n",
    "    print fn_(floatX([[1.0, 1.0],\n",
    "                      [1.0, 1.0],\n",
    "                      [1.0, 1.0]]),\n",
    "              floatX([[1.0, 1.0, 0.000001, 0.000001],\n",
    "                      [0.5, 0.5, 0.001, 0.001],\n",
    "                      [1.1, 0.9, 2.0, 0.5]]))\n",
    "    print '---'\n",
    "\n",
    "# sigma = T.scalar('sigma')\n",
    "# print (T.log(sigma)).eval({sigma: 0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a(x, y):\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = lambda y: a(3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
